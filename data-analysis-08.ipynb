{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-09T01:22:49.997487Z","iopub.execute_input":"2022-11-09T01:22:49.997975Z","iopub.status.idle":"2022-11-09T01:22:50.041259Z","shell.execute_reply.started":"2022-11-09T01:22:49.997876Z","shell.execute_reply":"2022-11-09T01:22:50.039288Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tourism-data/test/test.xlsx\n/kaggle/input/tourism-data/test/result1.csv\n/kaggle/input/tourism-data/train/2018-2019.xlsx\n/kaggle/input/tourism-data/train/2020-2021.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nimport jieba\nimport re\n%matplotlib inline\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:50.043252Z","iopub.execute_input":"2022-11-09T01:22:50.043901Z","iopub.status.idle":"2022-11-09T01:22:51.125572Z","shell.execute_reply.started":"2022-11-09T01:22:50.043865Z","shell.execute_reply":"2022-11-09T01:22:51.123957Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Task 1","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_excel('/kaggle/input/tourism-data/train/2018-2019.xlsx', sheet_name=4)\ndata2 = pd.read_excel('/kaggle/input/tourism-data/train/2020-2021.xlsx', sheet_name=4)\ntest_data = pd.read_excel('../input/tourism-data/test/test.xlsx')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:51.127367Z","iopub.execute_input":"2022-11-09T01:22:51.127737Z","iopub.status.idle":"2022-11-09T01:22:54.224925Z","shell.execute_reply.started":"2022-11-09T01:22:51.127698Z","shell.execute_reply":"2022-11-09T01:22:54.223914Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data1.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.227351Z","iopub.execute_input":"2022-11-09T01:22:54.227716Z","iopub.status.idle":"2022-11-09T01:22:54.245274Z","shell.execute_reply.started":"2022-11-09T01:22:54.227677Z","shell.execute_reply":"2022-11-09T01:22:54.244283Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   文章ID                        公众号标题              发布时间  \\\n0  1001                  2018，对自己好一点  2018-01-02 17:28   \n1  1002                    春节机票预订有窍门  2018-01-02 17:28   \n2  1003                      冬日旅游知多D  2018-01-03 17:32   \n3  1004                   2018冬季暖心之旅  2018-01-03 17:32   \n4  1005  关于粤K27618号大客车排气管“喷火”事件的情况说明  2018-01-05 16:57   \n\n                                                  正文  \n0  2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...  \n1  距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...  \n2  960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...  \n3  长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...  \n4                                                     ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>文章ID</th>\n      <th>公众号标题</th>\n      <th>发布时间</th>\n      <th>正文</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2018，对自己好一点</td>\n      <td>2018-01-02 17:28</td>\n      <td>2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>春节机票预订有窍门</td>\n      <td>2018-01-02 17:28</td>\n      <td>距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>冬日旅游知多D</td>\n      <td>2018-01-03 17:32</td>\n      <td>960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>2018冬季暖心之旅</td>\n      <td>2018-01-03 17:32</td>\n      <td>长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>关于粤K27618号大客车排气管“喷火”事件的情况说明</td>\n      <td>2018-01-05 16:57</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data2.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.246889Z","iopub.execute_input":"2022-11-09T01:22:54.247279Z","iopub.status.idle":"2022-11-09T01:22:54.257827Z","shell.execute_reply.started":"2022-11-09T01:22:54.247246Z","shell.execute_reply":"2022-11-09T01:22:54.256792Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   文章ID                                           公众号标题              发布时间  \\\n0  1622                            元旦晚8点，茂名海鲜亮相央视！一定要看！  2020-01-01 19:22   \n1  1623  建好省级全域旅游示范区，打造沿海经济带新增长极 茂名市电白区旅游协会举行第二届第二次全体会议  2020-01-05 02:10   \n2  1624                         【茂名交投旅游】@茂名人，信宜旅游景点上新啦~  2020-01-06 17:27   \n3  1625                          【茂名交投旅游】2020，打卡鼎龙湾新玩法！  2020-01-06 17:27   \n4  1626                              茂名 | 串珠成链，建滨海旅游目的地  2020-01-08 21:35   \n\n                                                  正文  \n0  新年伊始，你的打开方式是怎样的呢？是准备去享受一顿丰盛的大餐？还是将“世界那么大，我想去看看...  \n1  会员报到\\n\\n会场\\n\\n        为了更好地贯彻落实区委区政府和区文广旅体局建设全...  \n2  好消息！好消息！\\n\\n\\n信宜莲花湖庄园\\n\\n开业啦~~~\\n庄园简介\\n\\n\\n\\n\\...  \n3  2020.首波惊喜狂潮\\n鼎龙湾·德萨斯牛仔小镇全面升级\\n1月11日\\n牛仔主题嘉年华街区...  \n4  地处粤港澳大湾区、北部湾城市群、海南自贸区三大国家战略交汇处的茂名，紧紧抓住沿海经济带与大湾...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>文章ID</th>\n      <th>公众号标题</th>\n      <th>发布时间</th>\n      <th>正文</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1622</td>\n      <td>元旦晚8点，茂名海鲜亮相央视！一定要看！</td>\n      <td>2020-01-01 19:22</td>\n      <td>新年伊始，你的打开方式是怎样的呢？是准备去享受一顿丰盛的大餐？还是将“世界那么大，我想去看看...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1623</td>\n      <td>建好省级全域旅游示范区，打造沿海经济带新增长极 茂名市电白区旅游协会举行第二届第二次全体会议</td>\n      <td>2020-01-05 02:10</td>\n      <td>会员报到\\n\\n会场\\n\\n        为了更好地贯彻落实区委区政府和区文广旅体局建设全...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1624</td>\n      <td>【茂名交投旅游】@茂名人，信宜旅游景点上新啦~</td>\n      <td>2020-01-06 17:27</td>\n      <td>好消息！好消息！\\n\\n\\n信宜莲花湖庄园\\n\\n开业啦~~~\\n庄园简介\\n\\n\\n\\n\\...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1625</td>\n      <td>【茂名交投旅游】2020，打卡鼎龙湾新玩法！</td>\n      <td>2020-01-06 17:27</td>\n      <td>2020.首波惊喜狂潮\\n鼎龙湾·德萨斯牛仔小镇全面升级\\n1月11日\\n牛仔主题嘉年华街区...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1626</td>\n      <td>茂名 | 串珠成链，建滨海旅游目的地</td>\n      <td>2020-01-08 21:35</td>\n      <td>地处粤港澳大湾区、北部湾城市群、海南自贸区三大国家战略交汇处的茂名，紧紧抓住沿海经济带与大湾...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"columns = ['ID', 'Title', 'Date', 'Content']\ndata1.columns = columns\ndata2.columns = columns","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.259206Z","iopub.execute_input":"2022-11-09T01:22:54.260264Z","iopub.status.idle":"2022-11-09T01:22:54.265433Z","shell.execute_reply.started":"2022-11-09T01:22:54.260199Z","shell.execute_reply":"2022-11-09T01:22:54.264526Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data1, data2], ignore_index=True)\ndata['Content'] = data['Title'] + '\\n' + data['Content']\ndata.drop(columns=['Title', 'Date'], inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.266682Z","iopub.execute_input":"2022-11-09T01:22:54.267735Z","iopub.status.idle":"2022-11-09T01:22:54.317075Z","shell.execute_reply.started":"2022-11-09T01:22:54.267701Z","shell.execute_reply":"2022-11-09T01:22:54.316021Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     ID                                            Content\n0  1001  2018，对自己好一点\\n2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，...\n1  1002  春节机票预订有窍门\\n距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚...\n2  1003  冬日旅游知多D\\n960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\...\n3  1004  2018冬季暖心之旅\\n长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n...\n4  1005  关于粤K27618号大客车排气管“喷火”事件的情况说明\\n                 ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2018，对自己好一点\\n2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>春节机票预订有窍门\\n距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>冬日旅游知多D\\n960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>2018冬季暖心之旅\\n长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>关于粤K27618号大客车排气管“喷火”事件的情况说明\\n                 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.318563Z","iopub.execute_input":"2022-11-09T01:22:54.318991Z","iopub.status.idle":"2022-11-09T01:22:54.328597Z","shell.execute_reply.started":"2022-11-09T01:22:54.318956Z","shell.execute_reply":"2022-11-09T01:22:54.327523Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.330968Z","iopub.execute_input":"2022-11-09T01:22:54.331742Z","iopub.status.idle":"2022-11-09T01:22:54.344484Z","shell.execute_reply.started":"2022-11-09T01:22:54.331701Z","shell.execute_reply":"2022-11-09T01:22:54.343460Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   文章ID                                   公众号标题  \\\n0  8396  @茂名家长，广东全面启动3-11岁儿童新冠疫苗接种！是否安全？最全问答来了→   \n1  8397      庄悦群：把安全生产摆在首要位置抓紧抓实，加快推进绿色矿山智慧矿山建设   \n2  8398                            群众办事百项堵点疏解行动   \n3  8399             茂名启动台风Ⅲ级应急响应！市旅游局发布防御台风安全提示   \n4  8400                                  灾情就是命令   \n\n                                                  正文  \n0  按照国家整体部署\\n广东省全面启动\\n3-11岁人群新冠病毒疫苗接种\\n3-11岁人群接种哪...  \n1  10月31日上午，市委副书记、市长庄悦群带队到高州市，就深入贯彻落实习近平总书记关于安全生产...  \n2    为破解群众办事堵点难点，发展改革委会同新华社、中国政府网于4月19日启动了“群众办事百项...  \n3  点击上方蓝字，关注我们\\n\\n\\n台风最新消息\\n \\n目前，台风正朝着广东袭来\\n↓↓↓\\...  \n4  受南海热带低压影响，我市近日受强降雨影响，钱排、合水、新宝、平塘、思贺等镇遭受特大暴雨袭击，...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>文章ID</th>\n      <th>公众号标题</th>\n      <th>正文</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8396</td>\n      <td>@茂名家长，广东全面启动3-11岁儿童新冠疫苗接种！是否安全？最全问答来了→</td>\n      <td>按照国家整体部署\\n广东省全面启动\\n3-11岁人群新冠病毒疫苗接种\\n3-11岁人群接种哪...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8397</td>\n      <td>庄悦群：把安全生产摆在首要位置抓紧抓实，加快推进绿色矿山智慧矿山建设</td>\n      <td>10月31日上午，市委副书记、市长庄悦群带队到高州市，就深入贯彻落实习近平总书记关于安全生产...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8398</td>\n      <td>群众办事百项堵点疏解行动</td>\n      <td>为破解群众办事堵点难点，发展改革委会同新华社、中国政府网于4月19日启动了“群众办事百项...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8399</td>\n      <td>茂名启动台风Ⅲ级应急响应！市旅游局发布防御台风安全提示</td>\n      <td>点击上方蓝字，关注我们\\n\\n\\n台风最新消息\\n \\n目前，台风正朝着广东袭来\\n↓↓↓\\...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8400</td>\n      <td>灾情就是命令</td>\n      <td>受南海热带低压影响，我市近日受强降雨影响，钱排、合水、新宝、平塘、思贺等镇遭受特大暴雨袭击，...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.columns = ['ID', 'Title', 'Content']\ntest_data['Content'] = test_data['Title'] + '\\n' + test_data['Content']\ntest_data.drop(columns=['Title'], inplace=True)\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.348725Z","iopub.execute_input":"2022-11-09T01:22:54.348974Z","iopub.status.idle":"2022-11-09T01:22:54.362852Z","shell.execute_reply.started":"2022-11-09T01:22:54.348951Z","shell.execute_reply":"2022-11-09T01:22:54.361785Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"     ID                                            Content\n0  8396  @茂名家长，广东全面启动3-11岁儿童新冠疫苗接种！是否安全？最全问答来了→\\n按照国家整体...\n1  8397  庄悦群：把安全生产摆在首要位置抓紧抓实，加快推进绿色矿山智慧矿山建设\\n10月31日上午，市...\n2  8398  群众办事百项堵点疏解行动\\n  为破解群众办事堵点难点，发展改革委会同新华社、中国政府网于4...\n3  8399  茂名启动台风Ⅲ级应急响应！市旅游局发布防御台风安全提示\\n点击上方蓝字，关注我们\\n\\n\\n...\n4  8400  灾情就是命令\\n受南海热带低压影响，我市近日受强降雨影响，钱排、合水、新宝、平塘、思贺等镇遭...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8396</td>\n      <td>@茂名家长，广东全面启动3-11岁儿童新冠疫苗接种！是否安全？最全问答来了→\\n按照国家整体...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8397</td>\n      <td>庄悦群：把安全生产摆在首要位置抓紧抓实，加快推进绿色矿山智慧矿山建设\\n10月31日上午，市...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8398</td>\n      <td>群众办事百项堵点疏解行动\\n  为破解群众办事堵点难点，发展改革委会同新华社、中国政府网于4...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8399</td>\n      <td>茂名启动台风Ⅲ级应急响应！市旅游局发布防御台风安全提示\\n点击上方蓝字，关注我们\\n\\n\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8400</td>\n      <td>灾情就是命令\\n受南海热带低压影响，我市近日受强降雨影响，钱排、合水、新宝、平塘、思贺等镇遭...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Train data shape:', data.shape)\nprint('Test data shape:', test_data.shape)\ntrain_size = data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.364118Z","iopub.execute_input":"2022-11-09T01:22:54.364585Z","iopub.status.idle":"2022-11-09T01:22:54.372900Z","shell.execute_reply.started":"2022-11-09T01:22:54.364551Z","shell.execute_reply":"2022-11-09T01:22:54.371872Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Train data shape: (6285, 2)\nTest data shape: (930, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.concat([data, test_data], ignore_index=True)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.374582Z","iopub.execute_input":"2022-11-09T01:22:54.375346Z","iopub.status.idle":"2022-11-09T01:22:54.385224Z","shell.execute_reply.started":"2022-11-09T01:22:54.375311Z","shell.execute_reply":"2022-11-09T01:22:54.384105Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(7215, 2)"},"metadata":{}}]},{"cell_type":"code","source":"def preprocessing(data):\n    try:\n        lines = data.split()\n    except Exception as e:\n        print(data)\n        return ''\n    lines = list(filter(lambda x: x is not '', lines))\n    unuse_lis = []\n    rule_1 = r'\\W'\n    compiled_rule_1 = re.compile(rule_1)\n    for line in lines:\n        no_en_and_da = compiled_rule_1.findall(line)\n        no_en_and_da_str = ''.join(no_en_and_da)\n        reslis = re.findall(r'^\\S', ''.join(re.findall(r'[^\\，]', ''.join(re.findall(r'[^\\。]', no_en_and_da_str)))))\n        unuse_lis.append(reslis)\n    syms = []\n    for i in unuse_lis:\n        for j in i:\n            syms.append(j)\n    syms = list(set(syms))\n    \n    def replace_syms(line):\n        for sym in syms:\n            line = line.replace(sym, '')\n        return line\n    \n    def replace_lem(line):\n        a = re.sub(r'\\s', '', line)\n        b = re.sub(r'\\W{2,}', '', a)\n        c = re.sub(r'\\d', '', b)\n        d = re.sub(r' ', '', c)\n        d = d.replace('_', '')\n        return d\n    \n    lines = list(map(replace_syms, lines))\n    lines = list(map(replace_lem, lines))\n    lines = list(filter(lambda x: x not in '1234567890', lines))\n    \n    res = []\n    for line in lines:\n        for word in jieba.cut(line):\n            if word == '，':\n                continue\n            res.append(word)\n    return ' '.join(res)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.387042Z","iopub.execute_input":"2022-11-09T01:22:54.387549Z","iopub.status.idle":"2022-11-09T01:22:54.399613Z","shell.execute_reply.started":"2022-11-09T01:22:54.387516Z","shell.execute_reply":"2022-11-09T01:22:54.398687Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"preprocessing(data.loc[0, 'Content'])","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:54.400913Z","iopub.execute_input":"2022-11-09T01:22:54.401343Z","iopub.status.idle":"2022-11-09T01:22:55.254607Z","shell.execute_reply.started":"2022-11-09T01:22:54.401299Z","shell.execute_reply":"2022-11-09T01:22:55.253269Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.833 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'对 自己 好 一点 的 旅程 已经 结束 的 未来 拉开 了 帷幕 新 的 一年 里 请 对 自己 好 一点 一辈子 很长 漫漫 人生 路上 会 遇到 那么 多 的 人和事 一辈子 很 短 一年 一年 时光 悄悄 从 指缝间 溜走 在 有限 的 生命 里 从 现在 开始 享受 自己 的 人生 不 强求 缘分 该 遇见 的 人 总会 遇见 留不住 的 过客 终将 远走 有缘 相守 心怀 感恩 无缘 结交 坦然 放手 不 强求 你 我 都 是 自由 的 随缘 就 好 陪伴 家人 站 在 我们 身后 的 家人 是 前进 时 的 坚强 后盾 给予 我们 无条件 的 支持 是 孤独 无助 时 的 避风港 默默 付出 而 不求 回报 把 时间 留 一点 给 家人 他们 所 需 的 不过 是 陪伴 珍惜 朋友 时光流逝 仿佛 大浪淘沙 留下来 的 真心 朋友 不多不少 刚刚 好 三五 知己 小酌 几杯 君子之交 虽淡 但 贵在 真心 足矣 认真 工作 新 的 一年 里 全身心 投入 工作 付出 会 有 回报 工作 会为 你 搭 起 梦想 与 人生价值 之间 的 桥梁 认真 工作 别 辜负 自己 爱护 身体 健康 的 体魄 是 自己 最大 的 本钱 闲暇 时 散散步 锻炼 一下 身体 用 自律 而 健康 的 作息 对 自己 的 身体 负责 出去 走走 心灵 也 需要 放松 与 远行 城市 里 的 拥挤 高楼 令人 迷茫 不妨 出去 走走 行万里路 感受 万里 风光 体验 形色 人生 过去 已经 过去 未来 即将 到来 新 的 一年 愿 你 想要 的 明天 都 终将 到来 更 多 精彩 线路 欢迎 关注 茂名 交投 旅游 集散中心 站 前 四路 交委 车站 一楼 中心 联系人 林小姐 刘小姐 罗 小姐 陈小姐 市区 联系人 柯 小姐 柯 先生 吴小姐 梁小姐 邓 先生 电白 分部 蔡 小姐 高州 分部 唐小姐 化州 分部 马先生 李小姐 信宜 分部 陈小姐 谢小姐'"},"metadata":{}}]},{"cell_type":"code","source":"data['Cut'] = data['Content'].apply(preprocessing)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:22:55.256158Z","iopub.execute_input":"2022-11-09T01:22:55.256783Z","iopub.status.idle":"2022-11-09T01:23:39.062279Z","shell.execute_reply.started":"2022-11-09T01:22:55.256743Z","shell.execute_reply":"2022-11-09T01:23:39.061206Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     ID                                            Content  \\\n0  1001  2018，对自己好一点\\n2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，...   \n1  1002  春节机票预订有窍门\\n距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚...   \n2  1003  冬日旅游知多D\\n960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\...   \n3  1004  2018冬季暖心之旅\\n长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n...   \n4  1005  关于粤K27618号大客车排气管“喷火”事件的情况说明\\n                 ...   \n\n                                                 Cut  \n0  对 自己 好 一点 的 旅程 已经 结束 的 未来 拉开 了 帷幕 新 的 一年 里 请 对...  \n1  春节 机票 预订 有 窍门 距离 春节 还有 一个多月 的 时间 在线 旅游 网站 的 春节...  \n2  冬日 旅游 知多 D 万平方公里 的 祖国 大地 四季 都 有 独特 美景 冬天 的 旅行 ...  \n3  冬季 暖心 之 旅 长 按 二维码 关注 我们 中心 联系人 林小姐 刘小姐 罗 小姐 陈小...  \n4                   关于 粤 K 号 大客车 排气管 喷火 ” 事件 的 情况 说明  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Content</th>\n      <th>Cut</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2018，对自己好一点\\n2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，...</td>\n      <td>对 自己 好 一点 的 旅程 已经 结束 的 未来 拉开 了 帷幕 新 的 一年 里 请 对...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>春节机票预订有窍门\\n距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚...</td>\n      <td>春节 机票 预订 有 窍门 距离 春节 还有 一个多月 的 时间 在线 旅游 网站 的 春节...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>冬日旅游知多D\\n960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\...</td>\n      <td>冬日 旅游 知多 D 万平方公里 的 祖国 大地 四季 都 有 独特 美景 冬天 的 旅行 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>2018冬季暖心之旅\\n长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n...</td>\n      <td>冬季 暖心 之 旅 长 按 二维码 关注 我们 中心 联系人 林小姐 刘小姐 罗 小姐 陈小...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>关于粤K27618号大客车排气管“喷火”事件的情况说明\\n                 ...</td>\n      <td>关于 粤 K 号 大客车 排气管 喷火 ” 事件 的 情况 说明</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.063641Z","iopub.execute_input":"2022-11-09T01:23:39.064098Z","iopub.status.idle":"2022-11-09T01:23:39.070965Z","shell.execute_reply.started":"2022-11-09T01:23:39.064061Z","shell.execute_reply":"2022-11-09T01:23:39.070102Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(7215, 3)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1.1 By LDA","metadata":{}},{"cell_type":"code","source":"# data['AsList'] = data['Cut'].apply(lambda x: list(filter(lambda x: ' ' not in x, jieba.lcut(x))))\n# data['AsList'].head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.072315Z","iopub.execute_input":"2022-11-09T01:23:39.072815Z","iopub.status.idle":"2022-11-09T01:23:39.089550Z","shell.execute_reply.started":"2022-11-09T01:23:39.072781Z","shell.execute_reply":"2022-11-09T01:23:39.088533Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# text_data = data['AsList'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.090814Z","iopub.execute_input":"2022-11-09T01:23:39.091357Z","iopub.status.idle":"2022-11-09T01:23:39.100177Z","shell.execute_reply.started":"2022-11-09T01:23:39.091317Z","shell.execute_reply":"2022-11-09T01:23:39.099266Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# from gensim import corpora\n# from gensim.models import TfidfModel\n\n# dictionary = corpora.Dictionary(text_data)\n# dictionary.filter_n_most_frequent(200)\n# corpus = [dictionary.doc2bow(text) for text in text_data]\n\n# # tfidf = TfidfModel(corpus)\n# # tfidf.save('task_1_tfidf.model')\n# # # corpus = tfidf[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.101382Z","iopub.execute_input":"2022-11-09T01:23:39.101786Z","iopub.status.idle":"2022-11-09T01:23:39.111000Z","shell.execute_reply.started":"2022-11-09T01:23:39.101753Z","shell.execute_reply":"2022-11-09T01:23:39.110183Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# dictionary.save('task_1.dict')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.112428Z","iopub.execute_input":"2022-11-09T01:23:39.112783Z","iopub.status.idle":"2022-11-09T01:23:39.122552Z","shell.execute_reply.started":"2022-11-09T01:23:39.112750Z","shell.execute_reply":"2022-11-09T01:23:39.121571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# from gensim.models import LdaModel\n\n# # Set training parameters.\n# num_topics = 10\n# chunksize = 2000\n# passes = 20\n# iterations = 200\n# eval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# # Make a index to word dictionary.\n# temp = dictionary[0]  # This is only to \"load\" the dictionary.\n# id2word = dictionary.id2token\n\n# model = LdaModel(\n#     corpus=corpus,\n#     id2word=id2word,\n#     chunksize=chunksize,\n#     alpha='auto',\n#     eta='auto',\n#     iterations=iterations,\n#     num_topics=num_topics,\n#     passes=passes,\n#     eval_every=eval_every\n# )\n\n# model.save('task_1.model')  # 将模型保存到硬盘","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.124150Z","iopub.execute_input":"2022-11-09T01:23:39.124599Z","iopub.status.idle":"2022-11-09T01:23:39.136497Z","shell.execute_reply.started":"2022-11-09T01:23:39.124566Z","shell.execute_reply":"2022-11-09T01:23:39.135669Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# topic_list = model.print_topics()\n# topic_list = sorted(topic_list, key=lambda x: x[0])\n# print(topic_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.137752Z","iopub.execute_input":"2022-11-09T01:23:39.138190Z","iopub.status.idle":"2022-11-09T01:23:39.147855Z","shell.execute_reply.started":"2022-11-09T01:23:39.138147Z","shell.execute_reply":"2022-11-09T01:23:39.146925Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# top_topics = model.top_topics(corpus)\n\n# # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n# avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n# print('Average topic coherence: %.4f.' % avg_topic_coherence)\n\n# # from pprint import pprint\n# # pprint(top_topics)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.149404Z","iopub.execute_input":"2022-11-09T01:23:39.149743Z","iopub.status.idle":"2022-11-09T01:23:39.159592Z","shell.execute_reply.started":"2022-11-09T01:23:39.149708Z","shell.execute_reply":"2022-11-09T01:23:39.158514Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# lda_topics = []\n# for topic in top_topics:\n#     topic_list, _ = topic\n#     lda_topics.append([x[1] for x in topic_list])\n# lda_topics","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.160843Z","iopub.execute_input":"2022-11-09T01:23:39.162351Z","iopub.status.idle":"2022-11-09T01:23:39.170519Z","shell.execute_reply.started":"2022-11-09T01:23:39.162317Z","shell.execute_reply":"2022-11-09T01:23:39.169547Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# test_doc = text_data[23]\n# print(test_doc)\n# doc_bow = dictionary.doc2bow(test_doc)\n# # doc_tfidf = tfidf[doc_bow]\n# doc_lda = model[doc_bow]\n# doc_lda","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.171965Z","iopub.execute_input":"2022-11-09T01:23:39.172390Z","iopub.status.idle":"2022-11-09T01:23:39.181347Z","shell.execute_reply.started":"2022-11-09T01:23:39.172356Z","shell.execute_reply":"2022-11-09T01:23:39.180250Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# def judge_coherence(doc):\n#     coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n#                 '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n#                 '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n#                 '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n#                 '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n#     doc_bow = dictionary.doc2bow(test_doc)\n# #     doc_tfidf = tfidf[doc_bow]\n#     doc_lda = model[doc_bow]\n#     topic_idx = [each[0] for each in doc_lda]\n#     topics = []\n#     for idx in topic_idx:\n#         topics.extend(lda_topics[idx - 1])\n    \n#     def judge(topics, coh_dict):\n#         for x in topics:\n#             for y in coh_dict:\n#                 if y in x:\n#                     return True\n#         return False\n    \n#     return judge(topics, coh_dict)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.182868Z","iopub.execute_input":"2022-11-09T01:23:39.183266Z","iopub.status.idle":"2022-11-09T01:23:39.192315Z","shell.execute_reply.started":"2022-11-09T01:23:39.183233Z","shell.execute_reply":"2022-11-09T01:23:39.191261Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# test_text = text_data[train_size:]\n# test_res = [judge_coherence(text) for text in test_text]","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.194098Z","iopub.execute_input":"2022-11-09T01:23:39.194557Z","iopub.status.idle":"2022-11-09T01:23:39.206242Z","shell.execute_reply.started":"2022-11-09T01:23:39.194513Z","shell.execute_reply":"2022-11-09T01:23:39.205069Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 By TF-IDF and TextRank","metadata":{}},{"cell_type":"code","source":"import jieba.analyse\n\ndef judge_by_tfidf(text):\n    coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n                '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n                '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n                '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n                '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n    keywords = jieba.analyse.extract_tags(text, topK=50, withWeight=False)\n    for x in keywords:\n        for y in coh_dict:\n            if y in x:\n                return True\n    return False\n\ndef judge_by_textrank(text):\n    coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n                '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n                '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n                '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n                '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n    keywords = jieba.analyse.textrank(text, topK=50, withWeight=False)\n    for x in keywords:\n        for y in coh_dict:\n            if y in x:\n                return True\n    return False\n\ndef judge(text):\n    return '相关' if judge_by_tfidf(text) and judge_by_textrank(text) else '不相关'\n\ntest_res = [judge(text) for text in data[train_size:]['Cut'].tolist()]","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:23:39.212618Z","iopub.execute_input":"2022-11-09T01:23:39.212942Z","iopub.status.idle":"2022-11-09T01:24:00.387386Z","shell.execute_reply.started":"2022-11-09T01:23:39.212911Z","shell.execute_reply":"2022-11-09T01:24:00.386347Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"task1_res = pd.DataFrame({'文章ID': test_data['ID'].tolist(), \n                          '分类标签': test_res})\ntask1_res.to_csv('result1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:24:00.388642Z","iopub.execute_input":"2022-11-09T01:24:00.389480Z","iopub.status.idle":"2022-11-09T01:24:00.399634Z","shell.execute_reply.started":"2022-11-09T01:24:00.389444Z","shell.execute_reply":"2022-11-09T01:24:00.398663Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"markdown","source":"## Task 2.1","metadata":{}},{"cell_type":"code","source":"def read_in(path):\n    sheet0 = pd.read_excel(path, sheet_name=0)\n    sheet1 = pd.read_excel(path, sheet_name=1)\n    sheet2 = pd.read_excel(path, sheet_name=2)\n    sheet3 = pd.read_excel(path, sheet_name=3)\n    sheet0.columns = ['ID', 'City', 'Name', 'RevDate', 'Content', 'CheckDate', 'HouseType']\n    sheet0['ID'] = sheet0['ID'].apply(lambda x: '酒店评论-' + str(x))\n    sheet0 = sheet0[['ID', 'Content']]\n    sheet1.columns = ['ID', 'City', 'Name', 'Date', 'Content']\n    sheet1['ID'] = sheet1['ID'].apply(lambda x: '景区评论-' + str(x))\n    indices = sheet1[sheet1['Name'].str.contains('湛江|广东海洋大学|南极长城站')].index\n    sheet1.drop(index=indices, inplace=True)\n    sheet1 = sheet1[['ID', 'Content']]\n    sheet2 = sheet2[['游记ID', '正文']]\n    sheet2.columns = ['ID', 'Content']\n    sheet2['ID'] = sheet2['ID'].apply(lambda x: '游记-' + str(x))\n    sheet3 = sheet3[['餐饮评论ID', '评论内容', '标题']]\n    sheet3.columns = ['ID', 'Content', 'Title']\n    sheet3['Content'] = sheet3['Title'] + '\\n' + sheet3['Content']\n    sheet3.drop(columns='Title', inplace=True)\n    sheet3['ID'] = sheet3['ID'].apply(lambda x: '餐饮评论-' + str(x))\n    ret = pd.concat([sheet0, sheet1, sheet2, sheet3], ignore_index=True)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:24:00.400912Z","iopub.execute_input":"2022-11-09T01:24:00.401384Z","iopub.status.idle":"2022-11-09T01:24:00.412938Z","shell.execute_reply.started":"2022-11-09T01:24:00.401347Z","shell.execute_reply":"2022-11-09T01:24:00.412096Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data1 = read_in('../input/tourism-data/train/2018-2019.xlsx')\ndata2 = read_in('../input/tourism-data/train/2020-2021.xlsx')\nrev_data = pd.concat([data1, data2], ignore_index=True)\nrev_data.head(), rev_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:24:00.415045Z","iopub.execute_input":"2022-11-09T01:24:00.415625Z","iopub.status.idle":"2022-11-09T01:24:05.298787Z","shell.execute_reply.started":"2022-11-09T01:24:00.415591Z","shell.execute_reply":"2022-11-09T01:24:05.297710Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(          ID                                            Content\n 0  酒店评论-1001                                            干净卫生服务好\n 1  酒店评论-1002                                           环境可以，干净！\n 2  酒店评论-1003  环境不错，房间卫生都很好，生活也很方便，就是隔音效果不理想，有时太吵。我定的优惠价，性价比很...\n 3  酒店评论-1004                                    很好.......舒服态度不错\n 4  酒店评论-1005                                 #卫生# #设计风格# #酒店餐饮#,\n (9496, 2))"},"metadata":{}}]},{"cell_type":"code","source":"rev_data['Content'] = rev_data['Content'].apply(lambda x: re.sub(r'^.*?\\n\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\n.*?\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n| ', '', x))\nrev_data['Content'].dropna()\nrev_data.head(), rev_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:24:05.300414Z","iopub.execute_input":"2022-11-09T01:24:05.301054Z","iopub.status.idle":"2022-11-09T01:24:05.390704Z","shell.execute_reply.started":"2022-11-09T01:24:05.301013Z","shell.execute_reply":"2022-11-09T01:24:05.389882Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(          ID                                            Content\n 0  酒店评论-1001                                            干净卫生服务好\n 1  酒店评论-1002                                           环境可以，干净！\n 2  酒店评论-1003  环境不错，房间卫生都很好，生活也很方便，就是隔音效果不理想，有时太吵。我定的优惠价，性价比很...\n 3  酒店评论-1004                                    很好.......舒服态度不错\n 4  酒店评论-1005                                   #卫生##设计风格##酒店餐饮#,\n (9496, 2))"},"metadata":{}}]},{"cell_type":"code","source":"!pip install pyhanlp\n!pip install foolnltk\n!pip install tensorflow==1.14","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:24:05.392892Z","iopub.execute_input":"2022-11-09T01:24:05.393844Z","iopub.status.idle":"2022-11-09T01:25:28.085801Z","shell.execute_reply.started":"2022-11-09T01:24:05.393800Z","shell.execute_reply":"2022-11-09T01:25:28.084605Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Collecting pyhanlp\n  Downloading pyhanlp-0.1.84.tar.gz (136 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting jpype1==0.7.0\n  Downloading JPype1-0.7.0-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting hanlp-downloader\n  Downloading hanlp_downloader-0.0.25.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from hanlp-downloader->pyhanlp) (2.28.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->hanlp-downloader->pyhanlp) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->hanlp-downloader->pyhanlp) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->hanlp-downloader->pyhanlp) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->hanlp-downloader->pyhanlp) (3.3)\nBuilding wheels for collected packages: pyhanlp, hanlp-downloader\n  Building wheel for pyhanlp (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyhanlp: filename=pyhanlp-0.1.84-py3-none-any.whl size=29818 sha256=fccb87e3e30ed9770ff623607c5e31b9c57ba0bfb4ae872e6f75b4512718879b\n  Stored in directory: /root/.cache/pip/wheels/8e/67/74/f1dfe4d6ea994941a47aee14de63ca71aba2ab96e2e217c072\n  Building wheel for hanlp-downloader (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hanlp-downloader: filename=hanlp_downloader-0.0.25-py3-none-any.whl size=13776 sha256=46f93ee4f7643920a79cf304b9b2f829db44f144e530cbc21348645e7777eaa8\n  Stored in directory: /root/.cache/pip/wheels/cb/a2/c3/61f7b80f5d56e5b19dd9cb50303f8f3b621662f035a281d197\nSuccessfully built pyhanlp hanlp-downloader\nInstalling collected packages: jpype1, hanlp-downloader, pyhanlp\nSuccessfully installed hanlp-downloader-0.0.25 jpype1-0.7.0 pyhanlp-0.1.84\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting foolnltk\n  Downloading foolnltk-0.1.7.tar.gz (60.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from foolnltk) (2.6.4)\nRequirement already satisfied: numpy>=1.12.1 in /opt/conda/lib/python3.7/site-packages (from foolnltk) (1.21.6)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.43.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (0.4.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (5.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.6.3)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.15.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (3.3.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (0.15.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.12)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.1.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (3.19.4)\nCollecting numpy>=1.12.1\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.1.2)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (0.37.1)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (1.12.1)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (0.2.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (2.6.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.3.0->foolnltk) (2.6.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=1.3.0->foolnltk) (1.5.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (3.3.7)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (0.6.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (59.8.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (1.8.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (2.28.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (4.13.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=1.3.0->foolnltk) (3.2.0)\nBuilding wheels for collected packages: foolnltk\n  Building wheel for foolnltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for foolnltk: filename=foolnltk-0.1.7-py3-none-any.whl size=60814751 sha256=bec3db6ed4241400af624c4b1975de746c9cb3dda72988cbcc4c1b2cd4323888\n  Stored in directory: /root/.cache/pip/wheels/a0/c4/ff/97cdbe2f0ebec19828f3dd77b407bbc62d89ace6e64c0f83dc\nSuccessfully built foolnltk\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard, foolnltk\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed foolnltk-0.1.7 h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting tensorflow==1.14\n  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.3/109.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.43.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (3.19.4)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.15.0)\nRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.4.0)\nCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.5/488.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.2.0)\nRequirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.19.5)\nCollecting tensorboard<1.15.0,>=1.14.0\n  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting keras-applications>=1.0.6\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.15.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.37.1)\nCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.12.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.1.2)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (59.8.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.7)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.1.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.8.0)\nInstalling collected packages: tensorflow-estimator, astor, keras-applications, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.6.0\n    Uninstalling tensorflow-estimator-2.6.0:\n      Successfully uninstalled tensorflow-estimator-2.6.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.6.0\n    Uninstalling tensorboard-2.6.0:\n      Successfully uninstalled tensorboard-2.6.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.4\n    Uninstalling tensorflow-2.6.4:\n      Successfully uninstalled tensorflow-2.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 1.14.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 1.14.0 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 1.14.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 1.14.0 which is incompatible.\ntensorflow-decision-forests 0.2.0 requires tensorflow~=2.6, but you have tensorflow 1.14.0 which is incompatible.\ntensorflow-cloud 0.1.14 requires tensorboard>=2.3.0, but you have tensorboard 1.14.0 which is incompatible.\ntensorflow-cloud 0.1.14 requires tensorflow<3.0,>=1.15.0, but you have tensorflow 1.14.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 1.14.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\nexplainable-ai-sdk 1.3.3 requires tensorflow>=1.15.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed astor-0.8.1 keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from pyhanlp import *\nimport fool\n\nrev_data['Content'] = rev_data['Content'].apply(HanLP.convertToSimplifiedChinese)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:25:28.088373Z","iopub.execute_input":"2022-11-09T01:25:28.089109Z","iopub.status.idle":"2022-11-09T01:26:07.294254Z","shell.execute_reply.started":"2022-11-09T01:25:28.089069Z","shell.execute_reply":"2022-11-09T01:26:07.293255Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"下载 https://file.hankcs.com/hanlp/hanlp-1.8.3-release.zip 到 /opt/conda/lib/python3.7/site-packages/pyhanlp/static/hanlp-1.8.3-release.zip\n100%   1.8 MiB 361.9 KiB/s ETA:  0 s [=========================================]\n下载 https://file.hankcs.com/hanlp/data-for-1.7.5.zip 到 /opt/conda/lib/python3.7/site-packages/pyhanlp/static/data-for-1.8.3.zip\n100% 637.7 MiB  57.7 MiB/s ETA:  0 s [=========================================]\n                                                                                \r","output_type":"stream"},{"name":"stderr","text":"解压 data.zip...                                                                                      \n","output_type":"stream"}]},{"cell_type":"code","source":"task2_1 = pd.DataFrame(columns=['ID', 'pID', 'Name'])\nentity_data = pd.DataFrame(columns=['ID', 'Entities'])\nentity_dict = {}\nr = 1\ncnt = 1\n\ndef judge(entity: str):\n    if str.__contains__(entity, '国') or str.__contains__(entity, '市') or str.__contains__(entity, '区') or str.__contains__(entity, '州'):\n        return False\n    if entity in ['茂名', '湛江', '河东', '粤西', '水东']:\n        return False\n    if entity in ['河北', '山西', '辽宁', '吉林', '黑龙江', \n                  '江苏', '浙江', '安徽', '福建', '江西', \n                  '山东', '河南', '湖北', '湖南', '广东', \n                  '海南', '四川', '贵州', '云南', '陕西', \n                  '甘肃', '青海', '台湾', '内蒙古', '广西', \n                  '西藏', '宁夏', '新疆', '北京', '天津', \n                  '上海', '重庆', '香港', '澳门']:\n        return False\n    return True\n\nfor index, row in rev_data.iterrows():\n    ID, text = row['ID'], row['Content']\n    res = fool.analysis(text)[1][0]\n    if res != []:\n        entities = []\n        for each in res:\n            _, _, type_, entity = each\n            if type_ == 'location' and judge(entity):\n                entity = entity.strip()\n                entity = entity.replace(' ', '')\n                entity = entity.replace('\\n', '')\n                entities.append(entity)\n                if entity not in entity_dict:\n                    task2_1.loc[cnt - 1, 'ID'] = ID\n                    task2_1.loc[cnt - 1, 'pID'] = 'ID' + str(cnt)\n                    task2_1.loc[cnt - 1, 'Name'] = entity\n                    entity_dict[entity] = cnt\n                    cnt += 1\n                else:\n                    pID = entity_dict[entity]\n                    task2_1.loc[cnt - 1, 'ID'] = ID\n                    task2_1.loc[cnt - 1, 'pID'] = 'ID' + str(pID)\n                    task2_1.loc[cnt - 1, 'Name'] = entity\n        entity_data.loc[r, 'ID'] = ID\n        entity_data.loc[r, 'Entities'] = entities\n        r += 1\ntask2_1.columns = ['语料ID', '产品ID', '产品名称']\ntask2_1.to_csv('result2-1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:26:07.296258Z","iopub.execute_input":"2022-11-09T01:26:07.296833Z","iopub.status.idle":"2022-11-09T01:30:36.779639Z","shell.execute_reply.started":"2022-11-09T01:26:07.296795Z","shell.execute_reply":"2022-11-09T01:30:36.778564Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"2022-11-09 01:26:07.503444: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2022-11-09 01:26:07.507727: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz\n2022-11-09 01:26:07.508019: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555f8523ca60 executing computations on platform Host. Devices:\n2022-11-09 01:26:07.508045: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n2022-11-09 01:26:07.642008: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task 2.2","metadata":{}},{"cell_type":"code","source":"def read_in(path):\n    sheet1 = pd.read_excel(path, sheet_name=0)\n    sheet2 = pd.read_excel(path, sheet_name=1)\n    sheet3 = pd.read_excel(path, sheet_name=3)\n    sheet1['Label'] = '酒店'\n    hotels = sheet1[['酒店名称', 'Label']]\n    hotels.columns = ['Name', 'Label']\n    sheet2['Label'] = '景区'\n    views = sheet2[['景区名称', 'Label']]\n    views.columns = ['Name', 'Label']\n    sheet3['Label'] = '餐饮'\n    meals = sheet3[['餐饮名称', 'Label']]\n    meals.columns = ['Name', 'Label']\n    ret = pd.concat([hotels, views, meals], ignore_index=False)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:36.782313Z","iopub.execute_input":"2022-11-09T01:30:36.782957Z","iopub.status.idle":"2022-11-09T01:30:36.790584Z","shell.execute_reply.started":"2022-11-09T01:30:36.782886Z","shell.execute_reply":"2022-11-09T01:30:36.789608Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data1 = read_in('../input/tourism-data/train/2018-2019.xlsx')\ndata2 = read_in('../input/tourism-data/train/2020-2021.xlsx')\ntrain_data = pd.concat([data1, data2], ignore_index=True)\ntrain_data.head(), train_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:36.792278Z","iopub.execute_input":"2022-11-09T01:30:36.792700Z","iopub.status.idle":"2022-11-09T01:30:40.757981Z","shell.execute_reply.started":"2022-11-09T01:30:36.792659Z","shell.execute_reply":"2022-11-09T01:30:40.757070Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(             Name Label\n 0        茂名君悦商务酒店    酒店\n 1  维也纳国际酒店(茂名电白店)    酒店\n 2          茂名永利之家    酒店\n 3          茂名诚荟酒店    酒店\n 4        茂名华景商务酒店    酒店,\n (9280, 2))"},"metadata":{}}]},{"cell_type":"code","source":"train_data['Name'] = train_data['Name'].astype('str')\ntrain_data['Label'] = train_data['Label'].astype('str')\ntrain_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:40.759500Z","iopub.execute_input":"2022-11-09T01:30:40.759874Z","iopub.status.idle":"2022-11-09T01:30:40.778639Z","shell.execute_reply.started":"2022-11-09T01:30:40.759839Z","shell.execute_reply":"2022-11-09T01:30:40.777396Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9280 entries, 0 to 9279\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    9280 non-null   object\n 1   Label   9280 non-null   object\ndtypes: object(2)\nmemory usage: 145.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ntrain_data['Label'] = encoder.fit_transform(train_data['Label'])","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:40.780457Z","iopub.execute_input":"2022-11-09T01:30:40.780964Z","iopub.status.idle":"2022-11-09T01:30:40.790851Z","shell.execute_reply.started":"2022-11-09T01:30:40.780874Z","shell.execute_reply":"2022-11-09T01:30:40.789868Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\n\n# 划分为训练集和验证集\n# stratify 按照标签进行采样，训练集和验证部分同分布\nx_train, x_test, train_label, test_label =  train_test_split(train_data['Name'].tolist(), \n                      train_data['Label'].tolist(), test_size=0.3, stratify=train_data['Label'].tolist())","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:40.792951Z","iopub.execute_input":"2022-11-09T01:30:40.793999Z","iopub.status.idle":"2022-11-09T01:30:42.190414Z","shell.execute_reply.started":"2022-11-09T01:30:40.793964Z","shell.execute_reply":"2022-11-09T01:30:42.189449Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.4.0","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:42.192265Z","iopub.execute_input":"2022-11-09T01:30:42.193232Z","iopub.status.idle":"2022-11-09T01:30:57.381811Z","shell.execute_reply.started":"2022-11-09T01:30:42.193191Z","shell.execute_reply":"2022-11-09T01:30:57.380632Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Collecting transformers==4.4.0\n  Downloading transformers-4.4.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (0.0.53)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (1.19.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (4.13.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (4.64.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (2.28.1)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.4.0) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.4.0) (3.10.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.4.0) (3.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (3.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (8.0.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (1.0.1)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.10.3 transformers-4.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"USE_TF\"] = 'None'\n# transformers bert相关的模型使用和加载\nfrom transformers import BertTokenizer\n# 分词器，词典\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\ntrain_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=64)\ntest_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=64)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:57.384391Z","iopub.execute_input":"2022-11-09T01:30:57.385078Z","iopub.status.idle":"2022-11-09T01:30:59.779451Z","shell.execute_reply.started":"2022-11-09T01:30:57.385037Z","shell.execute_reply":"2022-11-09T01:30:59.778422Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dce1205e50a4ac1b2d6b125c9802d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7e08ad7b4a462d983fcf496b1694e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf3c4c7613c4918a9379538237988c2"}},"metadata":{}}]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    # 读取单个样本\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(int(self.labels[idx]))\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = MyDataset(train_encoding, train_label)\ntest_dataset = MyDataset(test_encoding, test_label)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:59.781078Z","iopub.execute_input":"2022-11-09T01:30:59.781484Z","iopub.status.idle":"2022-11-09T01:30:59.790219Z","shell.execute_reply.started":"2022-11-09T01:30:59.781449Z","shell.execute_reply":"2022-11-09T01:30:59.789276Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 单个读取到批量读取\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\n# 优化方法\noptim = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * 1\nscheduler = get_linear_schedule_with_warmup(optim, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:30:59.791426Z","iopub.execute_input":"2022-11-09T01:30:59.792289Z","iopub.status.idle":"2022-11-09T01:31:21.461265Z","shell.execute_reply.started":"2022-11-09T01:30:59.792252Z","shell.execute_reply":"2022-11-09T01:31:21.460287Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d52a44307904a008d3ca90ca173f749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d52f89db4c42fa9127af2edb4a9dc9"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:31:21.462682Z","iopub.execute_input":"2022-11-09T01:31:21.463084Z","iopub.status.idle":"2022-11-09T01:31:21.468725Z","shell.execute_reply.started":"2022-11-09T01:31:21.463046Z","shell.execute_reply":"2022-11-09T01:31:21.467767Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def train():\n    model.train()\n    total_train_loss = 0\n    iter_num = 0\n    total_iter = len(train_loader)\n    for batch in train_loader:\n        # 正向传播\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_train_loss += loss.item()\n        \n        # 反向梯度信息\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # 参数更新\n        optim.step()\n        scheduler.step()\n\n        iter_num += 1\n        if(iter_num % 100==0):\n            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num / total_iter * 100))\n        \n    print(\"Epoch: %d, Average training loss: %.4f\" % (epoch, total_train_loss / len(train_loader)))\n    \ndef validation():\n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    for batch in test_dataloader:\n        with torch.no_grad():\n            # 正常传播\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n\n        total_eval_loss += loss.item()\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n    print(\"Average testing loss: %.4f\" % (total_eval_loss / len(test_dataloader)))\n    print(\"-------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:31:21.470249Z","iopub.execute_input":"2022-11-09T01:31:21.470864Z","iopub.status.idle":"2022-11-09T01:31:21.483945Z","shell.execute_reply.started":"2022-11-09T01:31:21.470823Z","shell.execute_reply":"2022-11-09T01:31:21.483293Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\n\nfor epoch in range(num_epochs):\n    print(\"------------Epoch: %d ----------------\" % epoch)\n    train()\n    validation()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:31:21.485370Z","iopub.execute_input":"2022-11-09T01:31:21.485970Z","iopub.status.idle":"2022-11-09T01:32:19.696230Z","shell.execute_reply.started":"2022-11-09T01:31:21.485935Z","shell.execute_reply":"2022-11-09T01:32:19.695182Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"------------Epoch: 0 ----------------\nepoth: 0, iter_num: 100, loss: 0.0025, 24.63%\nepoth: 0, iter_num: 200, loss: 0.0009, 49.26%\nepoth: 0, iter_num: 300, loss: 0.0007, 73.89%\nepoth: 0, iter_num: 400, loss: 0.0007, 98.52%\nEpoch: 0, Average training loss: 0.0506\nAccuracy: 1.0000\nAverage testing loss: 0.0004\n-------------------------------\n------------Epoch: 1 ----------------\nepoth: 1, iter_num: 100, loss: 0.0007, 24.63%\nepoth: 1, iter_num: 200, loss: 0.0006, 49.26%\nepoth: 1, iter_num: 300, loss: 0.0007, 73.89%\nepoth: 1, iter_num: 400, loss: 0.0006, 98.52%\nEpoch: 1, Average training loss: 0.0007\nAccuracy: 1.0000\nAverage testing loss: 0.0004\n-------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"entities = list(entity_dict.keys())\n\nentity_encoding = tokenizer(entities, truncation=True, padding=True, max_length=64)\nentity_dataset = MyDataset(entity_encoding, np.zeros(len(entities)))\nentity_types = []\n\nentity_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nfor idx, batch in enumerate(entity_loader):\n    with torch.no_grad():\n        # 正常传播\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n        logits = logits.detach().cpu().numpy()\n        type_ = np.argmax(logits)\n        entity_types.append(type_)\nentity_types = encoder.inverse_transform(entity_types)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:32:19.700661Z","iopub.execute_input":"2022-11-09T01:32:19.706905Z","iopub.status.idle":"2022-11-09T01:32:42.827203Z","shell.execute_reply.started":"2022-11-09T01:32:19.706863Z","shell.execute_reply":"2022-11-09T01:32:42.826164Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def evaluate_counter(entity, data):\n    return len(data[data['Content'].str.contains(entity)].index)\n\nentity_evaluate = []\nfor entity in entities:\n    entity_evaluate.append(evaluate_counter(entity, rev_data))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:32:42.828874Z","iopub.execute_input":"2022-11-09T01:32:42.829381Z","iopub.status.idle":"2022-11-09T01:32:52.482991Z","shell.execute_reply.started":"2022-11-09T01:32:42.829326Z","shell.execute_reply":"2022-11-09T01:32:52.481996Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def essay_counter(entity, data):\n    return len(data[data['Cut'].str.contains(entity)].index)\n\nentity_essay = []\nfor entity in entities:\n    entity_essay.append(essay_counter(entity, data))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:32:52.484498Z","iopub.execute_input":"2022-11-09T01:32:52.485098Z","iopub.status.idle":"2022-11-09T01:33:10.349258Z","shell.execute_reply.started":"2022-11-09T01:32:52.485052Z","shell.execute_reply":"2022-11-09T01:33:10.348267Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"tour_data1 = pd.read_excel('../input/tourism-data/train/2018-2019.xlsx', sheet_name=2)\ntour_data2 = pd.read_excel('../input/tourism-data/train/2020-2021.xlsx', sheet_name=2)\ntour_data = pd.concat([tour_data1, tour_data2], ignore_index=True)\n\ndef tour_counter(entity, data):\n    return len(data[data['正文'].str.contains(entity)].index)\n\nentity_tour = []\nfor entity in entities:\n    entity_tour.append(tour_counter(entity, tour_data))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T01:36:45.840758Z","iopub.execute_input":"2022-11-09T01:36:45.843225Z","iopub.status.idle":"2022-11-09T01:36:50.600028Z","shell.execute_reply.started":"2022-11-09T01:36:45.843182Z","shell.execute_reply":"2022-11-09T01:36:50.598922Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}