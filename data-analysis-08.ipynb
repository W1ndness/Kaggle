{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-08T15:23:15.028912Z","iopub.execute_input":"2022-11-08T15:23:15.029293Z","iopub.status.idle":"2022-11-08T15:23:15.045911Z","shell.execute_reply.started":"2022-11-08T15:23:15.029260Z","shell.execute_reply":"2022-11-08T15:23:15.044819Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"/kaggle/input/tourism-data/test/test.xlsx\n/kaggle/input/tourism-data/test/result1.csv\n/kaggle/input/tourism-data/train/2018-2019.xlsx\n/kaggle/input/tourism-data/train/2020-2021.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nimport jieba\nimport re\n%matplotlib inline\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:16.479473Z","iopub.execute_input":"2022-11-08T15:23:16.479838Z","iopub.status.idle":"2022-11-08T15:23:16.486541Z","shell.execute_reply.started":"2022-11-08T15:23:16.479808Z","shell.execute_reply":"2022-11-08T15:23:16.485541Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# Task 1","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_excel('/kaggle/input/tourism-data/train/2018-2019.xlsx', sheet_name=4)\ndata2 = pd.read_excel('/kaggle/input/tourism-data/train/2020-2021.xlsx', sheet_name=4)\ntest_data = pd.read_excel('../input/tourism-data/test/test.xlsx')","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.370074Z","iopub.status.idle":"2022-11-08T14:12:20.373440Z","shell.execute_reply.started":"2022-11-08T14:12:20.373158Z","shell.execute_reply":"2022-11-08T14:12:20.373201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.374854Z","iopub.status.idle":"2022-11-08T14:12:20.375753Z","shell.execute_reply.started":"2022-11-08T14:12:20.375458Z","shell.execute_reply":"2022-11-08T14:12:20.375501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.377679Z","iopub.status.idle":"2022-11-08T14:12:20.379572Z","shell.execute_reply.started":"2022-11-08T14:12:20.379314Z","shell.execute_reply":"2022-11-08T14:12:20.379338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['ID', 'Title', 'Date', 'Content']\ndata1.columns = columns\ndata2.columns = columns","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.381737Z","iopub.status.idle":"2022-11-08T14:12:20.382417Z","shell.execute_reply.started":"2022-11-08T14:12:20.382136Z","shell.execute_reply":"2022-11-08T14:12:20.382161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data1, data2], ignore_index=True)\ndata['Content'] = data['Title'] + '\\n' + data['Content']\ndata.drop(columns=['Title', 'Date'], inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.384226Z","iopub.status.idle":"2022-11-08T14:12:20.385101Z","shell.execute_reply.started":"2022-11-08T14:12:20.384850Z","shell.execute_reply":"2022-11-08T14:12:20.384880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.386384Z","iopub.status.idle":"2022-11-08T14:12:20.390184Z","shell.execute_reply.started":"2022-11-08T14:12:20.389907Z","shell.execute_reply":"2022-11-08T14:12:20.389932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.391605Z","iopub.status.idle":"2022-11-08T14:12:20.396551Z","shell.execute_reply.started":"2022-11-08T14:12:20.396291Z","shell.execute_reply":"2022-11-08T14:12:20.396316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.columns = ['ID', 'Title', 'Content']\ntest_data['Content'] = test_data['Title'] + '\\n' + test_data['Content']\ntest_data.drop(columns=['Title'], inplace=True)\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.397856Z","iopub.status.idle":"2022-11-08T14:12:20.398542Z","shell.execute_reply.started":"2022-11-08T14:12:20.398289Z","shell.execute_reply":"2022-11-08T14:12:20.398313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train data shape:', data.shape)\nprint('Test data shape:', test_data.shape)\ntrain_size = data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.400225Z","iopub.status.idle":"2022-11-08T14:12:20.400895Z","shell.execute_reply.started":"2022-11-08T14:12:20.400645Z","shell.execute_reply":"2022-11-08T14:12:20.400670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data, test_data], ignore_index=True)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.402432Z","iopub.status.idle":"2022-11-08T14:12:20.403005Z","shell.execute_reply.started":"2022-11-08T14:12:20.402765Z","shell.execute_reply":"2022-11-08T14:12:20.402788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(data):\n    try:\n        lines = data.split()\n    except Exception as e:\n        print(data)\n        return ''\n    lines = list(filter(lambda x: x is not '', lines))\n    unuse_lis = []\n    rule_1 = r'\\W'\n    compiled_rule_1 = re.compile(rule_1)\n    for line in lines:\n        no_en_and_da = compiled_rule_1.findall(line)\n        no_en_and_da_str = ''.join(no_en_and_da)\n        reslis = re.findall(r'^\\S', ''.join(re.findall(r'[^\\，]', ''.join(re.findall(r'[^\\。]', no_en_and_da_str)))))\n        unuse_lis.append(reslis)\n    syms = []\n    for i in unuse_lis:\n        for j in i:\n            syms.append(j)\n    syms = list(set(syms))\n    \n    def replace_syms(line):\n        for sym in syms:\n            line = line.replace(sym, '')\n        return line\n    \n    def replace_lem(line):\n        a = re.sub(r'\\s', '', line)\n        b = re.sub(r'\\W{2,}', '', a)\n        c = re.sub(r'\\d', '', b)\n        d = re.sub(r' ', '', c)\n        d = d.replace('_', '')\n        return d\n    \n    lines = list(map(replace_syms, lines))\n    lines = list(map(replace_lem, lines))\n    lines = list(filter(lambda x: x not in '1234567890', lines))\n    \n    res = []\n    for line in lines:\n        for word in jieba.cut(line):\n            if word == '，':\n                continue\n            res.append(word)\n    return ' '.join(res)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.408994Z","iopub.status.idle":"2022-11-08T14:12:20.409636Z","shell.execute_reply.started":"2022-11-08T14:12:20.409308Z","shell.execute_reply":"2022-11-08T14:12:20.409331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessing(data.loc[0, 'Content'])","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.411120Z","iopub.status.idle":"2022-11-08T14:12:20.411731Z","shell.execute_reply.started":"2022-11-08T14:12:20.411490Z","shell.execute_reply":"2022-11-08T14:12:20.411513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Cut'] = data['Content'].apply(preprocessing)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.413465Z","iopub.status.idle":"2022-11-08T14:12:20.414011Z","shell.execute_reply.started":"2022-11-08T14:12:20.413763Z","shell.execute_reply":"2022-11-08T14:12:20.413786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.415754Z","iopub.status.idle":"2022-11-08T14:12:20.420577Z","shell.execute_reply.started":"2022-11-08T14:12:20.420273Z","shell.execute_reply":"2022-11-08T14:12:20.420298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 By LDA","metadata":{}},{"cell_type":"code","source":"data['AsList'] = data['Cut'].apply(lambda x: list(filter(lambda x: ' ' not in x, jieba.lcut(x))))\ndata['AsList'].head()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.421927Z","iopub.status.idle":"2022-11-08T14:12:20.422487Z","shell.execute_reply.started":"2022-11-08T14:12:20.422232Z","shell.execute_reply":"2022-11-08T14:12:20.422264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data = data['AsList'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.424392Z","iopub.status.idle":"2022-11-08T14:12:20.424934Z","shell.execute_reply.started":"2022-11-08T14:12:20.424687Z","shell.execute_reply":"2022-11-08T14:12:20.424710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim import corpora\nfrom gensim.models import TfidfModel\n\ndictionary = corpora.Dictionary(text_data)\ndictionary.filter_n_most_frequent(200)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\n\n# tfidf = TfidfModel(corpus)\n# tfidf.save('task_1_tfidf.model')\n# # corpus = tfidf[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.426444Z","iopub.status.idle":"2022-11-08T14:12:20.427072Z","shell.execute_reply.started":"2022-11-08T14:12:20.426813Z","shell.execute_reply":"2022-11-08T14:12:20.426839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary.save('task_1.dict')","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.432920Z","iopub.status.idle":"2022-11-08T14:12:20.433481Z","shell.execute_reply.started":"2022-11-08T14:12:20.433231Z","shell.execute_reply":"2022-11-08T14:12:20.433263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 200\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha='auto',\n    eta='auto',\n    iterations=iterations,\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every\n)\n\nmodel.save('task_1.model')  # 将模型保存到硬盘","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.435135Z","iopub.status.idle":"2022-11-08T14:12:20.435826Z","shell.execute_reply.started":"2022-11-08T14:12:20.435589Z","shell.execute_reply":"2022-11-08T14:12:20.435613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_list = model.print_topics()\ntopic_list = sorted(topic_list, key=lambda x: x[0])\nprint(topic_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.437349Z","iopub.status.idle":"2022-11-08T14:12:20.437957Z","shell.execute_reply.started":"2022-11-08T14:12:20.437709Z","shell.execute_reply":"2022-11-08T14:12:20.437733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_topics = model.top_topics(corpus)\n\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\n# from pprint import pprint\n# pprint(top_topics)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.439537Z","iopub.status.idle":"2022-11-08T14:12:20.447526Z","shell.execute_reply.started":"2022-11-08T14:12:20.447262Z","shell.execute_reply":"2022-11-08T14:12:20.447288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_topics = []\nfor topic in top_topics:\n    topic_list, _ = topic\n    lda_topics.append([x[1] for x in topic_list])\nlda_topics","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.448929Z","iopub.status.idle":"2022-11-08T14:12:20.449680Z","shell.execute_reply.started":"2022-11-08T14:12:20.449424Z","shell.execute_reply":"2022-11-08T14:12:20.449449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_doc = text_data[23]\nprint(test_doc)\ndoc_bow = dictionary.doc2bow(test_doc)\n# doc_tfidf = tfidf[doc_bow]\ndoc_lda = model[doc_bow]\ndoc_lda","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.451060Z","iopub.status.idle":"2022-11-08T14:12:20.451844Z","shell.execute_reply.started":"2022-11-08T14:12:20.451574Z","shell.execute_reply":"2022-11-08T14:12:20.451599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def judge_coherence(doc):\n    coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n                '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n                '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n                '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n                '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n    doc_bow = dictionary.doc2bow(test_doc)\n#     doc_tfidf = tfidf[doc_bow]\n    doc_lda = model[doc_bow]\n    topic_idx = [each[0] for each in doc_lda]\n    topics = []\n    for idx in topic_idx:\n        topics.extend(lda_topics[idx - 1])\n    \n    def judge(topics, coh_dict):\n        for x in topics:\n            for y in coh_dict:\n                if y in x:\n                    return True\n        return False\n    \n    return judge(topics, coh_dict)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.453313Z","iopub.status.idle":"2022-11-08T14:12:20.454107Z","shell.execute_reply.started":"2022-11-08T14:12:20.453833Z","shell.execute_reply":"2022-11-08T14:12:20.453858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = text_data[train_size:]\ntest_res = [judge_coherence(text) for text in test_text]","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.455907Z","iopub.status.idle":"2022-11-08T14:12:20.456717Z","shell.execute_reply.started":"2022-11-08T14:12:20.456458Z","shell.execute_reply":"2022-11-08T14:12:20.456482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 By TF-IDF and TextRank","metadata":{}},{"cell_type":"code","source":"import jieba.analyse\n\ndef judge_by_tfidf(text):\n    coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n                '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n                '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n                '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n                '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n    keywords = jieba.analyse.extract_tags(text, topK=50, withWeight=False)\n    for x in keywords:\n        for y in coh_dict:\n            if y in x:\n                return True\n    return False\n\ndef judge_by_textrank(text):\n    coh_dict = ['旅游', '活动', '节庆', '特产', '交通', '酒店', '景区', '景点',\n                '文创', '文化', '乡村旅游', '民宿', '假日', '假期', '游客', '采摘',\n                '赏花', '春游', '踏青', '康养', '公园', '滨海游', '度假', '农家乐',\n                '剧本杀', '旅行', '徒步', '工业旅游', '线路', '自驾游', '团队游',\n                '攻略', '游记', '包车', '玻璃栈道', '游艇', '高尔夫', '温泉']\n    keywords = jieba.analyse.textrank(text, topK=50, withWeight=False)\n    for x in keywords:\n        for y in coh_dict:\n            if y in x:\n                return True\n    return False\n\ndef judge(text):\n    return '相关' if judge_by_tfidf(text) and judge_by_textrank(text) else '不相关'\n\ntest_res = [judge(text) for text in data[train_size:]['Cut'].tolist()]","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.458190Z","iopub.status.idle":"2022-11-08T14:12:20.465541Z","shell.execute_reply.started":"2022-11-08T14:12:20.465272Z","shell.execute_reply":"2022-11-08T14:12:20.465299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"task1_res = pd.DataFrame({'文章ID': test_data['ID'].tolist(), \n                          '分类标签': test_res})\ntask1_res.to_csv('result1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:12:20.466952Z","iopub.status.idle":"2022-11-08T14:12:20.467736Z","shell.execute_reply.started":"2022-11-08T14:12:20.467476Z","shell.execute_reply":"2022-11-08T14:12:20.467502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"markdown","source":"## Task 2.1","metadata":{}},{"cell_type":"code","source":"def read_in(path):\n    sheet0 = pd.read_excel(path, sheet_name=0)\n    sheet1 = pd.read_excel(path, sheet_name=1)\n    sheet2 = pd.read_excel(path, sheet_name=2)\n    sheet3 = pd.read_excel(path, sheet_name=3)\n    sheet0.columns = ['ID', 'City', 'Name', 'RevDate', 'Content', 'CheckDate', 'HouseType']\n    sheet0['ID'] = sheet0['ID'].apply(lambda x: '酒店评论-' + str(x))\n    sheet0 = sheet0[['ID', 'Content']]\n    sheet1.columns = ['ID', 'City', 'Name', 'Date', 'Content']\n    sheet1['ID'] = sheet1['ID'].apply(lambda x: '景区评论-' + str(x))\n    indices = sheet1[sheet1['Name'].str.contains('湛江|广东海洋大学|南极长城站')].index\n    sheet1.drop(index=indices, inplace=True)\n    sheet1 = sheet1[['ID', 'Content']]\n    sheet2 = sheet2[['游记ID', '正文']]\n    sheet2.columns = ['ID', 'Content']\n    sheet2['ID'] = sheet2['ID'].apply(lambda x: '游记-' + str(x))\n    sheet3 = sheet3[['餐饮评论ID', '评论内容', '标题']]\n    sheet3.columns = ['ID', 'Content', 'Title']\n    sheet3['Content'] = sheet3['Title'] + '\\n' + sheet3['Content']\n    sheet3.drop(columns='Title', inplace=True)\n    sheet3['ID'] = sheet3['ID'].apply(lambda x: '餐饮评论-' + str(x))\n    ret = pd.concat([sheet0, sheet1, sheet2, sheet3], ignore_index=True)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:24.788755Z","iopub.execute_input":"2022-11-08T15:23:24.789120Z","iopub.status.idle":"2022-11-08T15:23:24.800452Z","shell.execute_reply.started":"2022-11-08T15:23:24.789088Z","shell.execute_reply":"2022-11-08T15:23:24.799412Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"data1 = read_in('../input/tourism-data/train/2018-2019.xlsx')\ndata2 = read_in('../input/tourism-data/train/2020-2021.xlsx')\ndata = pd.concat([data1, data2], ignore_index=True)\ndata.head(), data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:24.884624Z","iopub.execute_input":"2022-11-08T15:23:24.885449Z","iopub.status.idle":"2022-11-08T15:23:30.531050Z","shell.execute_reply.started":"2022-11-08T15:23:24.885403Z","shell.execute_reply":"2022-11-08T15:23:30.530021Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(          ID                                            Content\n 0  酒店评论-1001                                            干净卫生服务好\n 1  酒店评论-1002                                           环境可以，干净！\n 2  酒店评论-1003  环境不错，房间卫生都很好，生活也很方便，就是隔音效果不理想，有时太吵。我定的优惠价，性价比很...\n 3  酒店评论-1004                                    很好.......舒服态度不错\n 4  酒店评论-1005                                 #卫生# #设计风格# #酒店餐饮#,\n (9496, 2))"},"metadata":{}}]},{"cell_type":"code","source":"data['Content'] = data['Content'].apply(lambda x: re.sub(r'^.*?\\n\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\n.*?\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n| ', '', x))\ndata['Content'].dropna()\ndata.head(), data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:30.532925Z","iopub.execute_input":"2022-11-08T15:23:30.533566Z","iopub.status.idle":"2022-11-08T15:23:30.619322Z","shell.execute_reply.started":"2022-11-08T15:23:30.533527Z","shell.execute_reply":"2022-11-08T15:23:30.618307Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"(          ID                                            Content\n 0  酒店评论-1001                                            干净卫生服务好\n 1  酒店评论-1002                                           环境可以，干净！\n 2  酒店评论-1003  环境不错，房间卫生都很好，生活也很方便，就是隔音效果不理想，有时太吵。我定的优惠价，性价比很...\n 3  酒店评论-1004                                    很好.......舒服态度不错\n 4  酒店评论-1005                                   #卫生##设计风格##酒店餐饮#,\n (9496, 2))"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install pyhanlp\n# !pip install foolnltk\n# !pip install tensorflow==1.14","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:30.620835Z","iopub.execute_input":"2022-11-08T15:23:30.621294Z","iopub.status.idle":"2022-11-08T15:23:30.625758Z","shell.execute_reply.started":"2022-11-08T15:23:30.621258Z","shell.execute_reply":"2022-11-08T15:23:30.624659Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"from pyhanlp import *\nimport fool\n\ndata['Content'] = data['Content'].apply(HanLP.convertToSimplifiedChinese)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:23:30.628380Z","iopub.execute_input":"2022-11-08T15:23:30.628726Z","iopub.status.idle":"2022-11-08T15:23:30.963530Z","shell.execute_reply.started":"2022-11-08T15:23:30.628693Z","shell.execute_reply":"2022-11-08T15:23:30.962491Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"task2_1 = pd.DataFrame(columns=['ID', 'pID', 'Name'])\ncnt = 1\n\ndef judge(entity: str):\n    if str.__contains__(entity, '国') or str.__contains__(entity, '市') or str.__contains__(entity, '区') or str.__contains__(entity, '州'):\n        return False\n    if entity in ['茂名', '湛江', '河东', '粤西', '水东']:\n        return False\n    if entity in ['河北', '山西', '辽宁', '吉林', '黑龙江', \n                  '江苏', '浙江', '安徽', '福建', '江西', \n                  '山东', '河南', '湖北', '湖南', '广东', \n                  '海南', '四川', '贵州', '云南', '陕西', \n                  '甘肃', '青海', '台湾', '内蒙古', '广西', \n                  '西藏', '宁夏', '新疆', '北京', '天津', \n                  '上海', '重庆', '香港', '澳门']:\n        return False\n    return True\n\nfor index, row in data.iterrows():\n    ID, text = row['ID'], row['Content']\n    res = fool.analysis(text)[1][0]\n    if res != []:\n        for each in res:\n            _, _, type_, entity = each\n            if type_ == 'location' and judge(entity):\n                entity = entity.strip()\n                entity = entity.replace(' ', '')\n                entity = entity.replace('\\n', '')\n                task2_1.loc[cnt - 1, 'ID'] = ID\n                task2_1.loc[cnt - 1, 'pID'] = 'ID' + str(cnt)\n                task2_1.loc[cnt - 1, 'Name'] = entity\n                cnt += 1\ntask2_1.columns = ['语料ID', '产品ID', '产品名称']\ntask2_1.to_csv('result2-1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:56:45.688476Z","iopub.execute_input":"2022-11-08T14:56:45.688823Z","iopub.status.idle":"2022-11-08T15:01:01.281218Z","shell.execute_reply.started":"2022-11-08T14:56:45.688784Z","shell.execute_reply":"2022-11-08T15:01:01.280204Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"## Task 2.2","metadata":{}},{"cell_type":"code","source":"def read_in(path):\n    sheet1 = pd.read_excel(path, sheet_name=0)\n    sheet2 = pd.read_excel(path, sheet_name=1)\n    sheet3 = pd.read_excel(path, sheet_name=3)\n    sheet1['Label'] = '酒店'\n    hotels = sheet1[['酒店名称', 'Label']]\n    hotels.columns = ['Name', 'Label']\n    sheet2['Label'] = '景区'\n    views = sheet2[['景区名称', 'Label']]\n    views.columns = ['Name', 'Label']\n    sheet3['Label'] = '餐饮'\n    meals = sheet3[['餐饮名称', 'Label']]\n    meals.columns = ['Name', 'Label']\n    ret = pd.concat([hotels, views, meals], ignore_index=False)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:25:59.522717Z","iopub.execute_input":"2022-11-08T15:25:59.523113Z","iopub.status.idle":"2022-11-08T15:25:59.531594Z","shell.execute_reply.started":"2022-11-08T15:25:59.523080Z","shell.execute_reply":"2022-11-08T15:25:59.530230Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"data1 = read_in('../input/tourism-data/train/2018-2019.xlsx')\ndata2 = read_in('../input/tourism-data/train/2020-2021.xlsx')\ndata = pd.concat([data1, data2], ignore_index=True)\ndata.head(), data.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:25:59.630903Z","iopub.execute_input":"2022-11-08T15:25:59.631510Z","iopub.status.idle":"2022-11-08T15:26:03.700616Z","shell.execute_reply.started":"2022-11-08T15:25:59.631473Z","shell.execute_reply":"2022-11-08T15:26:03.699352Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"(             Name Label\n 0        茂名君悦商务酒店    酒店\n 1  维也纳国际酒店(茂名电白店)    酒店\n 2          茂名永利之家    酒店\n 3          茂名诚荟酒店    酒店\n 4        茂名华景商务酒店    酒店,\n (9280, 2))"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\n\n# 划分为训练集和验证集\n# stratify 按照标签进行采样，训练集和验证部分同分布\nx_train, x_test, train_label, test_label =  train_test_split(data['Name'], \n                      data['Label'], test_size=0.2, stratify=data['Label'])","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:27:31.144664Z","iopub.execute_input":"2022-11-08T15:27:31.145041Z","iopub.status.idle":"2022-11-08T15:27:31.172461Z","shell.execute_reply.started":"2022-11-08T15:27:31.145011Z","shell.execute_reply":"2022-11-08T15:27:31.171562Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.4.0","metadata":{"execution":{"iopub.status.busy":"2022-11-08T15:29:37.026867Z","iopub.execute_input":"2022-11-08T15:29:37.027258Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting transformers==4.4.0\n  Downloading transformers-4.4.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (0.0.53)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (4.64.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (1.19.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (2.28.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (4.13.0)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.4.0) (21.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.4.0) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.4.0) (3.10.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.4.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.4.0) (2022.9.24)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (8.0.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.4.0) (1.15.0)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n","output_type":"stream"}]},{"cell_type":"code","source":"# transformers bert相关的模型使用和加载\nfrom pytorch_transformers import BertTokenizer\n# 分词器，词典\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\ntrain_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=64)\ntest_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}